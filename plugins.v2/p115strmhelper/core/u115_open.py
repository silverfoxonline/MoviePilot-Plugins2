from concurrent.futures import ThreadPoolExecutor, Future, as_completed
from itertools import batched
from os import PathLike
from random import randint
from datetime import datetime, timezone
from pathlib import Path
from shutil import rmtree
from threading import Lock
from time import sleep, time, perf_counter
from typing import (
    Optional,
    Union,
    Literal,
    List,
    Tuple,
    Dict,
    Set,
    Any,
    Iterator,
    Iterable,
)

import oss2
import httpx
from sqlalchemy.orm.exc import MultipleResultsFound
from oss2 import SizedFileAdapter, determine_part_size
from oss2.models import PartInfo
from p115client import P115Client
from p115client.tool.attr import normalize_attr
from p115client.type import DirNode
from cryptography.hazmat.primitives import hashes
from diskcache import Deque

from app import schemas
from app.log import logger
from app.core.config import global_vars
from app.helper.storage import StorageHelper
from app.chain.storage import StorageChain
from app.modules.filemanager.storages import transfer_process
from app.schemas import NotificationType
from app.utils.string import StringUtils

from ..core.config import configer
from ..core.message import post_message
from ..core.cache import idpathcacher
from ..db_manager.oper import FileDbHelper, OpenFileOper
from ..utils.oopserver import OOPServerRequest
from ..utils.sentry import sentry_manager
from ..utils.exception import U115NoCheckInException, CanNotFindPathToCid
from ..utils.path import PathUtils
from ..utils.limiter import RateLimiter


p115_open_lock = Lock()


@sentry_manager.capture_all_class_exceptions
class U115OpenHelper:
    """
    115 Open Api
    """

    _auth_state = {}

    base_url = "https://proapi.115.com"

    chunk_size = 10 * 1024 * 1024

    retry_delay = 70

    def __init__(self):
        super().__init__()
        self.session = httpx.Client(follow_redirects=True, timeout=20.0)
        self._init_session()

        self.fail_upload_count = 0

        self.oopserver_request = OOPServerRequest(max_retries=3, backoff_factor=1.0)
        self.databasehelper = FileDbHelper()
        self.cookie_client = P115Client(configer.cookies)

    def _init_session(self):
        """
        åˆå§‹åŒ–
        """
        self.session.headers.update(
            {
                "User-Agent": "W115Storage/2.0",
                "Accept-Encoding": "gzip, deflate",
                "Content-Type": "application/x-www-form-urlencoded",
            }
        )

    def _check_session(self):
        """
        æ£€æŸ¥ä¼šè¯æ˜¯å¦è¿‡æœŸ
        """
        if not self.access_token:
            raise U115NoCheckInException("ã€P115Openã€‘è¯·å…ˆæ‰«ç ç™»å½•ï¼")

    @property
    def access_token(self) -> Optional[str]:
        """
        è®¿é—® token
        """
        with p115_open_lock:
            try:
                storagehelper = StorageHelper()
                u115_info = storagehelper.get_storage(storage="u115")
                if not u115_info:
                    return None
                tokens = u115_info.config
                refresh_token = tokens.get("refresh_token")
                if not refresh_token:
                    return None
                expires_in = tokens.get("expires_in", 0)
                refresh_time = tokens.get("refresh_time", 0)
                if expires_in and refresh_time + expires_in < int(time()):
                    tokens = self.__refresh_access_token(refresh_token)
                    if tokens:
                        storagehelper.set_storage(
                            storage="u115",
                            conf={"refresh_time": int(time()), **tokens},
                        )
                    else:
                        return None
                access_token = tokens.get("access_token")
                if access_token:
                    self.session.headers.update(
                        {"Authorization": f"Bearer {access_token}"}
                    )
                return access_token
            except Exception as e:
                logger.error(f"ã€P115Openã€‘è·å–è®¿é—® Token å‡ºç°æœªçŸ¥é”™è¯¯: {e}")
                return None

    def __refresh_access_token(self, refresh_token: str) -> Optional[dict]:
        """
        åˆ·æ–° access_token

        :param refresh_token: åˆ·æ–°ä»¤ç‰Œ

        :return: æ•°æ®å­—å…¸
        """
        resp = self.session.post(
            "https://passportapi.115.com/open/refreshToken",
            data={"refresh_token": refresh_token},
        )
        if resp is None:
            logger.error(
                f"ã€P115Openã€‘åˆ·æ–° access_token å¤±è´¥ï¼šrefresh_token={refresh_token}"
            )
            return None
        result = resp.json()
        if result.get("code") != 0:
            logger.warn(
                f"ã€P115Openã€‘åˆ·æ–° access_token å¤±è´¥ï¼š{result.get('code')} - {result.get('message')}ï¼"
            )
            return None
        return result.get("data")

    def _request_api(
        self,
        method: str,
        endpoint: str,
        result_key: Optional[str] = None,
        **kwargs,
    ) -> Optional[Union[dict, list]]:
        """
        API è¯·æ±‚å‡½æ•°
        """
        # æ£€æŸ¥ä¼šè¯
        self._check_session()

        # é”™è¯¯æ—¥å¿—æ ‡å¿—
        no_error_log = kwargs.pop("no_error_log", False)
        # é‡è¯•æ¬¡æ•°
        retry_times = kwargs.pop("retry_limit", 5)

        try:
            resp = self.session.request(method, f"{self.base_url}{endpoint}", **kwargs)
        except httpx.RequestError as e:
            logger.error(f"ã€P115Openã€‘{method} è¯·æ±‚ {endpoint} ç½‘ç»œé”™è¯¯: {str(e)}")
            return None

        if resp is None:
            logger.warn(f"ã€P115Openã€‘{method} è¯·æ±‚ {endpoint} å¤±è´¥ï¼")
            return None

        kwargs["retry_limit"] = retry_times

        # å¤„ç†é€Ÿç‡é™åˆ¶
        if resp.status_code == 429:
            reset_time = 5 + int(resp.headers.get("X-RateLimit-Reset", 60))
            logger.debug(
                f"ã€P115Openã€‘{method} è¯·æ±‚ {endpoint} é™æµï¼Œç­‰å¾…{reset_time}ç§’åé‡è¯•"
            )
            sleep(reset_time)
            return self._request_api(method, endpoint, result_key, **kwargs)

        # å¤„ç†è¯·æ±‚é”™è¯¯
        try:
            resp.raise_for_status()
        except httpx.HTTPStatusError as e:
            if retry_times <= 0:
                logger.error(
                    f"ã€P115Openã€‘{method} è¯·æ±‚ {endpoint} é”™è¯¯ {e}ï¼Œé‡è¯•æ¬¡æ•°ç”¨å°½ï¼"
                )
                return None
            kwargs["retry_limit"] = retry_times - 1
            sleep_duration = 2 ** (5 - retry_times + 1)
            logger.info(
                f"ã€P115Openã€‘{method} è¯·æ±‚ {endpoint} é”™è¯¯ {e}ï¼Œç­‰å¾… {sleep_duration} ç§’åé‡è¯•..."
            )
            sleep(sleep_duration)
            return self._request_api(method, endpoint, result_key, **kwargs)

        # è¿”å›æ•°æ®
        ret_data = resp.json()
        if ret_data.get("code") != 0:
            error_msg = ret_data.get("message")
            if not no_error_log:
                logger.warn(f"ã€P115Openã€‘{method} è¯·æ±‚ {endpoint} å‡ºé”™ï¼š{error_msg}")
            if "å·²è¾¾åˆ°å½“å‰è®¿é—®ä¸Šé™" in error_msg:
                if retry_times <= 0:
                    logger.error(
                        f"ã€P115Openã€‘{method} è¯·æ±‚ {endpoint} è¾¾åˆ°è®¿é—®ä¸Šé™ï¼Œé‡è¯•æ¬¡æ•°ç”¨å°½ï¼"
                    )
                    return None
                kwargs["retry_limit"] = retry_times - 1
                logger.info(
                    f"ã€P115Openã€‘{method} è¯·æ±‚ {endpoint} è¾¾åˆ°è®¿é—®ä¸Šé™ï¼Œç­‰å¾… {self.retry_delay} ç§’åé‡è¯•..."
                )
                sleep(self.retry_delay)
                return self._request_api(method, endpoint, result_key, **kwargs)
            return None

        if result_key:
            return ret_data.get(result_key)
        return ret_data

    @staticmethod
    def _delay_get_item(path: Path) -> Optional[schemas.FileItem]:
        """
        è‡ªåŠ¨å»¶è¿Ÿé‡è¯• get_item æ¨¡å—

        :param path: è·¯å¾„
        """
        storage_chain = StorageChain()
        for i in range(1, 4):
            sleep(2**i)
            file_item = storage_chain.get_file_item(storage="u115", path=Path(path))
            if file_item:
                return file_item
        return None

    def get_download_url(
        self,
        pickcode: str,
        user_agent: str,
    ) -> Optional[str]:
        """
        è·å–ä¸‹è½½é“¾æ¥

        :param pickcode: æå–ç 
        :param user_agent: è¯·æ±‚ UA å¤´
        """
        download_info = self._request_api(
            "POST",
            "/open/ufile/downurl",
            "data",
            data={"pick_code": pickcode},
            headers={"User-Agent": user_agent},
        )
        if not download_info:
            return None
        logger.debug(f"ã€P115Openã€‘è·å–åˆ°ä¸‹è½½ä¿¡æ¯: {download_info}")
        try:
            return list(download_info.values())[0].get("url", {}).get("url")
        except Exception as e:
            logger.error(f"ã€P115Openã€‘è§£æä¸‹è½½é“¾æ¥å¤±è´¥: {e}")
            return None

    @staticmethod
    def _calc_sha1(filepath: Path, size: Optional[int] = None) -> str:
        """
        è®¡ç®—æ–‡ä»¶ SHA1

        :param filepath: æ–‡ä»¶è·¯å¾„
        :param size: å‰å¤šå°‘å­—èŠ‚
        """
        sha1 = hashes.Hash(hashes.SHA1())
        with open(filepath, "rb") as f:
            if size:
                chunk = f.read(size)
                sha1.update(chunk)
            else:
                while chunk := f.read(8192):
                    sha1.update(chunk)
        return sha1.finalize().hex()

    @staticmethod
    def _can_write_db(path: Path) -> bool:
        """
        åˆ¤æ–­ç›®å½•æ˜¯å¦èƒ½å†™å…¥æ•°æ®åº“

        :param path: è·¯å¾„
        """
        # å­˜åœ¨å¾…æ•´ç†ç›®å½•æ—¶ï¼Œåˆ¤æ–­éå¾…æ•´ç†ç›®å½•æ‰å†™å…¥ï¼Œä¸å­˜åœ¨å¾…æ•´ç†ç›®å½•ç›´æ¥å†™å…¥æ•°æ®åº“
        if configer.pan_transfer_paths:
            if not PathUtils.get_run_transfer_path(
                configer.pan_transfer_paths, path.as_posix()
            ):
                return True
        else:
            return True
        return False

    def upload_fail_count(self) -> bool:
        """
        ä¸Šä¼ é‡è¯•åˆ¤æ–­
        """
        if self.fail_upload_count == 0:
            self.fail_upload_count += 1
            return True
        else:
            self.fail_upload_count = 0
            return False

    def upload(
        self,
        target_dir: schemas.FileItem,
        local_path: Path,
        new_name: Optional[str] = None,
    ) -> Optional[schemas.FileItem]:
        """
        æ–‡ä»¶ä¸Šä¼ 
        """

        def encode_callback(cb: str) -> str:
            return oss2.utils.b64encode_as_string(cb)

        def send_upload_info(
            file_sha1: Optional[str],
            first_sha1: Optional[str],
            second_auth: bool,
            second_sha1: Optional[str],
            file_size: Optional[str],
            file_name: Optional[str],
            upload_time: Optional[int],
        ):
            """
            å‘é€ä¸Šä¼ ä¿¡æ¯
            """
            path = "/upload/info"
            headers = {"x-machine-id": configer.get_config("MACHINE_ID")}
            json_data = {
                "file_sha1": file_sha1,
                "first_sha1": first_sha1,
                "second_auth": second_auth,
                "second_sha1": second_sha1,
                "file_size": file_size,
                "file_name": file_name,
                "time": upload_time,
                "postime": datetime.now(timezone.utc)
                .isoformat(timespec="milliseconds")
                .replace("+00:00", "Z"),
            }
            try:
                response = self.oopserver_request.make_request(
                    path=path,
                    method="POST",
                    headers=headers,
                    json_data=json_data,
                    timeout=10.0,
                )

                if response is not None and response.status_code == 201:
                    logger.info(
                        f"ã€P115Openã€‘ä¸Šä¼ ä¿¡æ¯æŠ¥å‘ŠæœåŠ¡å™¨æˆåŠŸ: {response.json()}"
                    )
                else:
                    logger.warn("ã€P115Openã€‘ä¸Šä¼ ä¿¡æ¯æŠ¥å‘ŠæœåŠ¡å™¨å¤±è´¥ï¼Œç½‘ç»œé—®é¢˜")
            except Exception as e:
                logger.warn(f"ã€P115Openã€‘ä¸Šä¼ ä¿¡æ¯æŠ¥å‘ŠæœåŠ¡å™¨å¤±è´¥: {e}")

        def send_upload_wait(target_name):
            """
            å‘é€ä¸Šä¼ ç­‰å¾…
            """
            if configer.notify and configer.upload_module_notify:
                post_message(
                    mtype=NotificationType.Plugin,
                    title="ã€115ç½‘ç›˜ã€‘ä¸Šä¼ æ¨¡å—å¢å¼º",
                    text=f"\nè§¦å‘ç§’ä¼ ç­‰å¾…ï¼š{target_name}\n",
                )

            try:
                self.oopserver_request.make_request(
                    path="/upload/wait",
                    method="POST",
                    headers={"x-machine-id": configer.get_config("MACHINE_ID")},
                    timeout=10.0,
                )
            except Exception:
                pass

        target_name = new_name or local_path.name
        target_path = Path(target_dir.path) / target_name
        # è®¡ç®—æ–‡ä»¶ç‰¹å¾å€¼
        file_size = local_path.stat().st_size
        file_sha1 = self._calc_sha1(local_path)
        file_preid = self._calc_sha1(local_path, 128 * 1024 * 1024)

        # è·å–ç›®æ ‡ç›®å½•CID
        target_cid = target_dir.fileid
        target_param = f"U_1_{target_cid}"

        wait_start_time = perf_counter()
        send_wait = False
        while True:
            start_time = perf_counter()
            # Step 1: åˆå§‹åŒ–ä¸Šä¼ 
            init_data = {
                "file_name": target_name,
                "file_size": file_size,
                "target": target_param,
                "fileid": file_sha1,
                "preid": file_preid,
            }
            init_resp = self._request_api(
                "POST", "/open/upload/init", data=init_data, timeout=120.0
            )
            if not init_resp:
                return None
            if not init_resp.get("state"):
                logger.warn(f"ã€P115Openã€‘åˆå§‹åŒ–ä¸Šä¼ å¤±è´¥: {init_resp.get('error')}")
                return None
            # ç»“æœ
            init_result = init_resp.get("data")
            logger.debug(f"ã€P115Openã€‘ä¸Šä¼  Step 1 åˆå§‹åŒ–ç»“æœ: {init_result}")
            # å›è°ƒä¿¡æ¯
            bucket_name = init_result.get("bucket")
            object_name = init_result.get("object")
            callback = init_result.get("callback")
            # äºŒæ¬¡è®¤è¯ä¿¡æ¯
            sign_check = init_result.get("sign_check")
            pick_code = init_result.get("pick_code")
            sign_key = init_result.get("sign_key")

            # Step 2: å¤„ç†äºŒæ¬¡è®¤è¯
            second_auth = False
            second_sha1 = ""
            if init_result.get("code") in [700, 701] and sign_check:
                second_auth = True
                sign_checks = sign_check.split("-")
                start = int(sign_checks[0])
                end = int(sign_checks[1])
                # è®¡ç®—æŒ‡å®šåŒºé—´çš„SHA1
                # sign_check ï¼ˆç”¨ä¸‹åˆ’çº¿éš”å¼€,æˆªå–ä¸Šä¼ æ–‡å†…å®¹çš„sha1ï¼‰(å•ä½æ˜¯byte): "2392148-2392298"
                with open(local_path, "rb") as f:
                    # å–2392148-2392298ä¹‹é—´çš„å†…å®¹(åŒ…å«2392148ã€2392298)çš„sha1
                    f.seek(start)
                    chunk = f.read(end - start + 1)
                    sha1 = hashes.Hash(hashes.SHA1())
                    sha1.update(chunk)
                    sign_val = sha1.finalize().hex().upper()
                second_sha1 = sign_val
                # é‡æ–°åˆå§‹åŒ–è¯·æ±‚
                # sign_keyï¼Œsign_val(æ ¹æ®sign_checkè®¡ç®—çš„å€¼å¤§å†™çš„sha1å€¼)
                init_data.update(
                    {"pick_code": pick_code, "sign_key": sign_key, "sign_val": sign_val}
                )
                init_resp = self._request_api(
                    "POST", "/open/upload/init", data=init_data, timeout=120.0
                )
                if not init_resp:
                    return None
                if not init_resp.get("state"):
                    logger.warn(
                        f"ã€P115Openã€‘å¤„ç†äºŒæ¬¡è®¤è¯å¤±è´¥: {init_resp.get('error')}"
                    )
                    return None
                # äºŒæ¬¡è®¤è¯ç»“æœ
                init_result = init_resp.get("data")
                logger.debug(f"ã€P115Openã€‘ä¸Šä¼  Step 2 äºŒæ¬¡è®¤è¯ç»“æœ: {init_result}")
                if not pick_code:
                    pick_code = init_result.get("pick_code")
                if not bucket_name:
                    bucket_name = init_result.get("bucket")
                if not object_name:
                    object_name = init_result.get("object")
                if not callback:
                    callback = init_result.get("callback")

            # Step 3: ç§’ä¼ 
            if init_result.get("status") == 2:
                logger.info(f"ã€P115Openã€‘{target_name} ç§’ä¼ æˆåŠŸ")
                end_time = perf_counter()
                elapsed_time = end_time - start_time
                send_upload_info(
                    file_sha1,
                    file_preid,
                    second_auth,
                    second_sha1,
                    str(file_size),
                    target_name,
                    int(elapsed_time),
                )
                file_id = init_result.get("file_id", None)
                if file_id:
                    logger.debug(
                        f"ã€P115Openã€‘{target_name} ä½¿ç”¨ç§’ä¼ è¿”å›IDè·å–æ–‡ä»¶ä¿¡æ¯"
                    )
                    sleep(2)
                    info_resp = self._request_api(
                        "GET",
                        "/open/folder/get_info",
                        "data",
                        params={"file_id": int(file_id)},
                        timeout=120.0,
                    )
                    if info_resp:
                        try:
                            return schemas.FileItem(
                                storage="u115",
                                fileid=str(info_resp["file_id"]),
                                path=str(target_path)
                                + ("/" if info_resp["file_category"] == "0" else ""),
                                type="file"
                                if info_resp["file_category"] == "1"
                                else "dir",
                                name=info_resp["file_name"],
                                basename=Path(info_resp["file_name"]).stem,
                                extension=Path(info_resp["file_name"])
                                .suffix[1:]
                                .lower()
                                if info_resp["file_category"] == "1"
                                else None,
                                pickcode=info_resp["pick_code"],
                                size=StringUtils.num_filesize(info_resp["size"])
                                if info_resp["file_category"] == "1"
                                else None,
                                modify_time=info_resp["utime"],
                            )
                        except Exception as e:
                            logger.error(f"ã€P115Openã€‘å¤„ç†è¿”å›ä¿¡æ¯å¤±è´¥: {e}")
                            return None
                return U115OpenHelper._delay_get_item(target_path)

            # åˆ¤æ–­æ˜¯ç­‰å¾…ç§’ä¼ è¿˜æ˜¯ç›´æ¥ä¸Šä¼ 
            upload_module_skip_upload_wait_size = int(
                configer.get_config("upload_module_skip_upload_wait_size") or 0
            )
            if (
                upload_module_skip_upload_wait_size != 0
                and file_size <= upload_module_skip_upload_wait_size
            ):
                logger.info(
                    f"ã€P115Openã€‘æ–‡ä»¶å¤§å° {file_size} å°äºæœ€ä½é˜ˆå€¼ï¼Œè·³è¿‡ç­‰å¾…æµç¨‹: {target_name}"
                )
                break

            if perf_counter() - wait_start_time > int(
                configer.get_config("upload_module_wait_timeout")
            ):
                logger.warn(
                    f"ã€P115Openã€‘ç­‰å¾…ç§’ä¼ è¶…æ—¶ï¼Œè‡ªåŠ¨è¿›è¡Œä¸Šä¼ æµç¨‹: {target_name}"
                )
                break

            upload_module_force_upload_wait_size = int(
                configer.get_config("upload_module_force_upload_wait_size") or 0
            )
            if (
                upload_module_force_upload_wait_size != 0
                and file_size >= upload_module_force_upload_wait_size
            ):
                logger.info(
                    f"ã€P115Openã€‘æ–‡ä»¶å¤§å° {file_size} å¤§äºæœ€é«˜é˜ˆå€¼ï¼Œå¼ºåˆ¶ç­‰å¾…æµç¨‹: {target_name}"
                )
                sleep(int(configer.get_config("upload_module_wait_time")))
            else:
                try:
                    response = self.oopserver_request.make_request(
                        path="/speed/user_status/me",
                        method="GET",
                        headers={"x-machine-id": configer.get_config("MACHINE_ID")},
                        timeout=10.0,
                    )

                    if response is not None and response.status_code == 200:
                        resp = response.json()
                        if resp.get("status") != "slow":
                            logger.warn(
                                f"ã€P115Openã€‘ä¸Šä¼ é€Ÿåº¦çŠ¶æ€ {resp.get('status')}ï¼Œè·³è¿‡ç§’ä¼ ç­‰å¾…: {target_name}"
                            )
                            break

                        # è®¡ç®—ç­‰å¾…æ—¶é—´
                        default_wait_time = int(
                            configer.get_config("upload_module_wait_time")
                        )
                        sleep_time = default_wait_time
                        fastest_speed = resp.get("fastest_user_speed_mbps", None)
                        user_speed = resp.get("user_average_speed_mbps", None)
                        if fastest_speed and user_speed:
                            bs = user_speed * 0.2 + fastest_speed * 0.8
                            wt = file_size / (1024 * 1024) / bs
                            if wt > 10 * 60:
                                wt = wt / (wt // (10 * 60) + 1)
                            if wt <= default_wait_time // 2:
                                wt += default_wait_time // 2
                            sleep_time = int(wt)

                        logger.info(
                            f"ã€P115Openã€‘ä¼‘çœ  {sleep_time} ç§’ï¼Œç­‰å¾…ç§’ä¼ : {target_name}"
                        )
                        if not send_wait:
                            send_upload_wait(target_name)
                            send_wait = True
                        sleep(sleep_time)
                    else:
                        logger.warn("ã€P115Openã€‘è·å–ç”¨æˆ·ä¸Šä¼ é€Ÿåº¦é”™è¯¯ï¼Œç½‘ç»œé—®é¢˜")
                        break
                except Exception as e:
                    logger.warn(f"ã€P115Openã€‘è·å–ç”¨æˆ·ä¸Šä¼ é€Ÿåº¦é”™è¯¯: {e}")
                    break

        if configer.upload_module_skip_slow_upload:
            logger.warn(
                f"ã€P115Openã€‘{target_name} æ— æ³•ç§’ä¼ ï¼Œè·³è¿‡ä¸Šä¼  {configer.upload_module_skip_slow_upload}"
            )
            return None

        # Step 4: è·å–ä¸Šä¼ å‡­è¯
        second_auth = False
        token_resp = self._request_api(
            "GET", "/open/upload/get_token", "data", timeout=120.0
        )
        if not token_resp:
            logger.warn("ã€P115Openã€‘è·å–ä¸Šä¼ å‡­è¯å¤±è´¥")
            return None
        logger.debug(f"ã€P115Openã€‘ä¸Šä¼  Step 4 è·å–ä¸Šä¼ å‡­è¯ç»“æœ: {token_resp}")
        # ä¸Šä¼ å‡­è¯
        endpoint = token_resp.get("endpoint")
        access_key_id = token_resp.get("AccessKeyId")
        access_key_secret = token_resp.get("AccessKeySecret")
        security_token = token_resp.get("SecurityToken")

        # Step 5: æ–­ç‚¹ç»­ä¼ 
        resume_resp = self._request_api(
            "POST",
            "/open/upload/resume",
            "data",
            data={
                "file_size": file_size,
                "target": target_param,
                "fileid": file_sha1,
                "pick_code": pick_code,
            },
            timeout=120.0,
        )
        if resume_resp:
            logger.debug(f"ã€P115Openã€‘ä¸Šä¼  Step 5 æ–­ç‚¹ç»­ä¼ ç»“æœ: {resume_resp}")
            if resume_resp.get("callback"):
                callback = resume_resp["callback"]

        # Step 6: å¯¹è±¡å­˜å‚¨ä¸Šä¼ 
        auth = oss2.StsAuth(
            access_key_id=access_key_id,
            access_key_secret=access_key_secret,
            security_token=security_token,
        )
        bucket = oss2.Bucket(auth, endpoint, bucket_name, connect_timeout=120)  # noqa
        part_size = determine_part_size(file_size, preferred_size=self.chunk_size)

        # åˆå§‹åŒ–è¿›åº¦æ¡
        logger.info(
            f"ã€P115Openã€‘å¼€å§‹ä¸Šä¼ : {local_path} -> {target_path}ï¼Œåˆ†ç‰‡å¤§å°ï¼š{StringUtils.str_filesize(part_size)}"
        )
        progress_callback = transfer_process(local_path.as_posix())

        try:
            # åˆå§‹åŒ–åˆ†ç‰‡
            upload_id = None
            for attempt in range(3):
                try:
                    upload_id = bucket.init_multipart_upload(
                        object_name, params={"encoding-type": "url", "sequential": ""}
                    ).upload_id
                    break
                except Exception as e:
                    logger.warn(
                        f"ã€P115Openã€‘åˆå§‹åŒ–åˆ†ç‰‡ä¸Šä¼ å¤±è´¥: {e}ï¼Œæ­£åœ¨é‡è¯•... ({attempt + 1}/3)"
                    )
                    sleep(2**attempt)

            if not upload_id:
                logger.error(
                    f"ã€P115Openã€‘{target_name} åˆå§‹åŒ–åˆ†ç‰‡ä¸Šä¼ æœ€ç»ˆå¤±è´¥ï¼Œä¸Šä¼ ç»ˆæ­¢ã€‚"
                )
                return None

            parts = []
            # é€ä¸ªä¸Šä¼ åˆ†ç‰‡
            with open(local_path, "rb") as fileobj:
                part_number = 1
                offset = 0
                while offset < file_size:
                    if global_vars.is_transfer_stopped(local_path.as_posix()):
                        logger.info(f"ã€P115Openã€‘{local_path} ä¸Šä¼ å·²å–æ¶ˆï¼")
                        return None
                    num_to_upload = min(part_size, file_size - offset)
                    for attempt in range(3):
                        try:
                            # æ¯æ¬¡é‡è¯•æ—¶ï¼Œéƒ½éœ€è¦å°†æ–‡ä»¶æŒ‡é’ˆç§»åˆ°å½“å‰åˆ†ç‰‡çš„èµ·å§‹ä½ç½®
                            fileobj.seek(offset)

                            logger.info(
                                f"ã€P115Openã€‘å¼€å§‹ä¸Šä¼  {target_name} åˆ†ç‰‡ {part_number}: {offset} -> {offset + num_to_upload}"
                            )
                            result = bucket.upload_part(
                                object_name,
                                upload_id,
                                part_number,
                                data=SizedFileAdapter(fileobj, num_to_upload),
                            )
                            parts.append(PartInfo(part_number, result.etag))
                            logger.info(
                                f"ã€P115Openã€‘{target_name} åˆ†ç‰‡ {part_number} ä¸Šä¼ å®Œæˆ"
                            )
                            break
                        except oss2.exceptions.OssError as e:
                            # åˆ¤æ–­æ˜¯å¦ä¸ºSTS Tokenè¿‡æœŸé”™è¯¯
                            if e.code == "SecurityTokenExpired":
                                logger.warn(
                                    f"ã€P115Openã€‘ä¸Šä¼ å‡­è¯å·²è¿‡æœŸï¼Œæ­£åœ¨é‡æ–°è·å–... (é‡è¯•æ¬¡æ•°: {attempt + 1}/3)"
                                )
                                # Step 4: é‡æ–°è·å–ä¸Šä¼ å‡­è¯
                                token_resp = self._request_api(
                                    "GET",
                                    "/open/upload/get_token",
                                    "data",
                                    timeout=120.0,
                                )
                                if not token_resp:
                                    logger.error(
                                        "ã€P115Openã€‘é‡æ–°è·å–ä¸Šä¼ å‡­è¯å¤±è´¥ï¼Œä¸Šä¼ ç»ˆæ­¢ã€‚"
                                    )
                                    return None

                                # æ›´æ–°OSSå®¢æˆ·ç«¯çš„è®¤è¯ä¿¡æ¯
                                access_key_id = token_resp.get("AccessKeyId")
                                access_key_secret = token_resp.get("AccessKeySecret")
                                security_token = token_resp.get("SecurityToken")
                                auth = oss2.StsAuth(
                                    access_key_id=access_key_id,
                                    access_key_secret=access_key_secret,
                                    security_token=security_token,
                                )
                                bucket = oss2.Bucket(
                                    auth,  # noqa
                                    endpoint,
                                    bucket_name,
                                    connect_timeout=120,
                                )
                                logger.info(
                                    "ã€P115Openã€‘ä¸Šä¼ å‡­è¯å·²åˆ·æ–°ï¼Œå°†é‡è¯•å½“å‰åˆ†ç‰‡ã€‚"
                                )
                                continue
                            logger.warn(
                                f"ã€P115Openã€‘ä¸Šä¼ åˆ†ç‰‡ {part_number} å¤±è´¥: {e}ï¼Œæ­£åœ¨é‡è¯•... ({attempt + 1}/3)"
                            )
                            sleep(2**attempt)
                        except Exception as e:
                            logger.warn(
                                f"ã€P115Openã€‘ä¸Šä¼ åˆ†ç‰‡ {part_number} å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}ï¼Œæ­£åœ¨é‡è¯•... ({attempt + 1}/3)"
                            )
                            sleep(2**attempt)
                    else:
                        logger.error(
                            f"ã€P115Openã€‘{target_name} åˆ†ç‰‡ {part_number} è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä¸Šä¼ ç»ˆæ­¢ã€‚"
                        )
                        return None

                    offset += num_to_upload
                    part_number += 1
                    # æ›´æ–°è¿›åº¦
                    progress = (offset * 100) / file_size
                    progress_callback(progress)
        except Exception as e:
            logger.error(f"ã€P115Openã€‘{target_name} åˆ†å—ç”Ÿæˆå‡ºç°æœªçŸ¥é”™è¯¯: {e}")
            return None
        else:
            # å®Œæˆä¸Šä¼ 
            progress_callback(100)

        # è¯·æ±‚å¤´
        headers = {
            "X-oss-callback": encode_callback(callback["callback"]),
            "x-oss-callback-var": encode_callback(callback["callback_var"]),
            "x-oss-forbid-overwrite": "false",
        }
        try:
            result = bucket.complete_multipart_upload(
                object_name, upload_id, parts, headers=headers
            )
            if result.status == 200:
                try:
                    data = result.resp.response.json()
                    logger.debug(f"ã€P115Openã€‘ä¸Šä¼  Step 6 å›è°ƒç»“æœï¼š{data}")
                    if data.get("state") is False:
                        if self.upload_fail_count():
                            logger.warn(f"ã€P115Openã€‘{target_name} ä¸Šä¼ é‡è¯•")
                            return self.upload(target_dir, local_path, new_name)
                        logger.error(f"ã€P115Openã€‘{target_name} ä¸Šä¼ å¤±è´¥")
                        return None
                except Exception:
                    logger.warn("ã€P115Openã€‘ä¸Šä¼  Step 6 å›è°ƒæ— ç»“æœ")
                logger.info(f"ã€P115Openã€‘{target_name} ä¸Šä¼ æˆåŠŸ")
            else:
                logger.warn(
                    f"ã€P115Openã€‘{target_name} ä¸Šä¼ å¤±è´¥ï¼Œé”™è¯¯ç : {result.status}"
                )
                return None
        except oss2.exceptions.OssError as e:
            if e.code == "InvalidAccessKeyId":
                logger.warn(
                    f"ã€P115Openã€‘ä¸Šä¼ å‡­è¯å¤±æ•ˆï¼Œå°†é‡æ–°è·å–å‡­è¯å¹¶ç»§ç»­ä¸Šä¼ : {target_name}"
                )
                return self.upload(target_dir, local_path, new_name)

            if e.code == "FileAlreadyExists":
                logger.warn(f"ã€P115Openã€‘{target_name} å·²å­˜åœ¨")
            else:
                logger.error(
                    f"ã€P115Openã€‘{target_name} ä¸Šä¼ å¤±è´¥: {e.status}, é”™è¯¯ç : {e.code}, è¯¦æƒ…: {e.message}"
                )
                return None
        except Exception as e:
            logger.error(f"ã€P115Openã€‘{target_name} å›è°ƒå‡ºç°æœªçŸ¥é”™è¯¯: {e}")
            return None

        end_time = perf_counter()
        elapsed_time = end_time - start_time
        send_upload_info(
            file_sha1,
            file_preid,
            second_auth,
            second_sha1,
            str(file_size),
            target_name,
            int(elapsed_time),
        )
        # è¿”å›ç»“æœ
        return U115OpenHelper._delay_get_item(target_path)

    def create_folder(
        self, parent_item: schemas.FileItem, name: str
    ) -> Optional[schemas.FileItem]:
        """
        åˆ›å»ºç›®å½•

        Cookie / OpenAPI éšæœºè½®æ¢
        """
        new_path = Path(parent_item.path) / name

        if randint(0, 1) == 0:
            resp = self._request_api(
                "POST",
                "/open/folder/add",
                data={"pid": int(parent_item.fileid or "0"), "file_name": name},
            )
            if not resp:
                return None
            if not resp.get("state"):
                if resp.get("code") == 20004:
                    # ç›®å½•å·²å­˜åœ¨
                    return self.get_item(new_path)
                logger.warn(f"ã€P115Openã€‘åˆ›å»ºç›®å½•å¤±è´¥: {resp.get('error')}")
                return None
            logger.debug(f"ã€P115Openã€‘OpenAPI åˆ›å»ºç›®å½•: {new_path}")
            try:
                return schemas.FileItem(
                    storage="u115",
                    fileid=str(resp["data"]["file_id"]),
                    path=new_path.as_posix() + "/",
                    name=name,
                    basename=name,
                    type="dir",
                    modify_time=int(time()),
                )
            except Exception as e:
                logger.error(f"ã€P115Openã€‘å¤„ç†è¿”å›ä¿¡æ¯å¤±è´¥: {e}")
                return None
        else:
            resp = self.cookie_client.fs_mkdir(name, pid=int(parent_item.fileid or "0"))
            if not resp.get("state"):
                if resp.get("errno") == 20004:
                    return self.get_item(new_path)
                logger.warn(f"ã€P115Openã€‘åˆ›å»ºç›®å½•å¤±è´¥: {resp}")
                return None
            logger.debug(f"ã€P115Openã€‘Cookie åˆ›å»ºç›®å½•: {new_path}")
            try:
                return schemas.FileItem(
                    storage="u115",
                    fileid=str(resp["cid"]),
                    path=new_path.as_posix() + "/",
                    name=name,
                    basename=name,
                    type="dir",
                    modify_time=int(time()),
                )
            except Exception as e:
                logger.error(f"ã€P115Openã€‘å¤„ç†è¿”å›ä¿¡æ¯å¤±è´¥: {e}")
                return None

    def open_get_item(self, path: Path) -> Optional[schemas.FileItem]:
        """
        è·å–æŒ‡å®šè·¯å¾„çš„æ–‡ä»¶/ç›®å½•é¡¹ OpenAPI
        """
        try:
            resp = self._request_api(
                "POST",
                "/open/folder/get_info",
                "data",
                data={"path": path.as_posix()},
                no_error_log=True,
            )
            if not resp:
                return None
            logger.debug(f"ã€P115Openã€‘OpenAPI è·å–æ–‡ä»¶ä¿¡æ¯ {path} {resp['file_id']}")
            file_item = schemas.FileItem(
                storage="u115",
                fileid=str(resp["file_id"]),
                path=path.as_posix() + ("/" if resp["file_category"] == "0" else ""),
                type="file" if resp["file_category"] == "1" else "dir",
                name=resp["file_name"],
                basename=Path(resp["file_name"]).stem,
                extension=Path(resp["file_name"]).suffix[1:]
                if resp["file_category"] == "1"
                else None,
                pickcode=resp["pick_code"],
                size=resp["size_byte"] if resp["file_category"] == "1" else None,
                modify_time=resp["utime"],
            )
            if self._can_write_db(path):
                self.databasehelper.upsert_batch(
                    self.databasehelper.process_fileitem(file_item)
                )
            return file_item
        except Exception as e:
            logger.debug(f"ã€P115Openã€‘OpenAPI è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}")
            return None

    def database_get_item(self, path: Path) -> Optional[schemas.FileItem]:
        """
        è·å–æŒ‡å®šè·¯å¾„çš„æ–‡ä»¶/ç›®å½•é¡¹ DataBase
        """
        try:
            try:
                data = self.databasehelper.get_by_path(path=path.as_posix())
            except MultipleResultsFound:
                return None
            if data:
                if data.get("id", None):
                    logger.debug(
                        f"ã€P115Openã€‘DataBase è·å–æ–‡ä»¶ä¿¡æ¯ {path} {data.get('id')}"
                    )
                    return schemas.FileItem(
                        storage="u115",
                        fileid=str(data.get("id")),
                        path=path.as_posix()
                        + ("/" if data.get("type") == "folder" else ""),
                        type="file" if data.get("type") == "file" else "dir",
                        name=data.get("name"),
                        basename=Path(data.get("name")).stem,
                        extension=Path(data.get("name")).suffix[1:]
                        if data.get("type") == "file"
                        else None,
                        pickcode=data.get("pickcode", ""),
                        size=data.get("size") if data.get("type") == "file" else None,
                        modify_time=data.get("mtime", 0),
                    )
            return None
        except Exception as e:
            logger.debug(f"ã€P115Openã€‘DataBase è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}")
            return None

    def cookie_get_item(self, path: Path) -> Optional[schemas.FileItem]:
        """
        è·å–æŒ‡å®šè·¯å¾„çš„ç›®å½•é¡¹ Cookie

        1. ç¼“å­˜è¯»å– ï½œ Cookie æ¥å£è·å–ç›®å½• ID
        2. è·å–æ–‡ä»¶è¯¦ç»†ä¿¡æ¯
        """
        try:
            if path.name != path.stem:
                return None
            if path.as_posix() == "/":
                folder_id = 0
            else:
                cache_id = idpathcacher.get_id_by_dir(directory=path.as_posix())
                if cache_id:
                    folder_id = cache_id
                else:
                    payload = {
                        "path": path.as_posix(),
                    }
                    resp = self.cookie_client.fs_dir_getid(payload)
                    if not resp.get("state", None):
                        return None
                    folder_id = resp.get("id", None)
                    if not folder_id or folder_id == 0:
                        return None
            sleep(1)
            resp = self.cookie_client.fs_file(folder_id)
            if not resp.get("state", None):
                return None
            data: list = resp.get("data", [])
            if not data:
                return None
            data: dict = data[0]
            data["path"] = path.as_posix()
            if self._can_write_db(path):
                self.databasehelper.upsert_batch(
                    self.databasehelper.process_fs_files_item(data)
                )
            logger.debug(f"ã€P115Openã€‘Cookie è·å–æ–‡ä»¶ä¿¡æ¯ {path} {data.get('cid')}")
            return schemas.FileItem(
                storage="u115",
                fileid=str(data.get("cid")),
                path=path.as_posix() + "/",
                type="dir",
                name=data.get("n"),
                basename=Path(data.get("n")).stem,
                extension=None,
                pickcode=data.get("pc"),
                size=None,
                modify_time=data.get("t"),
            )
        except Exception as e:
            logger.debug(f"ã€P115Openã€‘Cookie è·å–æ–‡ä»¶å¤¹ä¿¡æ¯å¤±è´¥: {str(e)}")
            return None

    def get_item(self, path: Path) -> Optional[schemas.FileItem]:
        """
        è·å–æŒ‡å®šè·¯å¾„çš„æ–‡ä»¶/ç›®å½•é¡¹

        1. æ•°æ®åº“è·å–
        2. OpenAPI æ¥å£è·å–
        """
        db_item = self.database_get_item(path)
        if db_item:
            return db_item
        return self.open_get_item(path)

    def get_folder(self, path: Path) -> Optional[schemas.FileItem]:
        """
        è·å–æŒ‡å®šè·¯å¾„çš„æ–‡ä»¶å¤¹ï¼Œå¦‚ä¸å­˜åœ¨åˆ™åˆ›å»º
        """
        folder = self.get_item(path)
        if folder:
            return folder

        try:
            resp = self.cookie_client.fs_makedirs_app(path.as_posix(), pid=0)
            if not resp.get("state"):
                logger.error(f"ã€P115Openã€‘{path} ç›®å½•åˆ›å»ºå¤±è´¥ï¼š{resp}")
                return None
            idpathcacher.add_cache(id=int(resp["cid"]), directory=path.as_posix())
            return self.cookie_get_item(path)
        except Exception as e:
            logger.error(f"ã€P115Openã€‘{path} ç›®å½•åˆ›å»ºå‡ºç°æœªçŸ¥é”™è¯¯ï¼š{e}")
            return None

    def rename(self, fileitem: schemas.FileItem, name: str) -> bool:
        """
        é‡å‘½åæ–‡ä»¶/ç›®å½•

        Cookie / OpenAPI éšæœºè½®æ¢
        """
        r = randint(0, 2)
        if r == 0:
            resp = self._request_api(
                "POST",
                "/open/ufile/update",
                data={"file_id": int(fileitem.fileid), "file_name": name},
            )
        elif r == 1:
            resp = self.cookie_client.fs_rename((int(fileitem.fileid), name))
        else:
            resp = self.cookie_client.fs_rename_app((int(fileitem.fileid), name))
        if not resp:
            return False
        if resp.get("state"):
            return True
        return False

    def recyclebin_clean(self, payload: int | str | Iterable[int | str] | dict = "", /):
        """
        æ¸…ç†å›æ”¶ç«™

        .. note::
            åªè¦ä¸æŒ‡å®š `tid`ï¼Œå°±ä¼šæ¸…ç©ºå›æ”¶ç«™

        :payload:
            - tid: int | str = "" ğŸ’¡ å¤šä¸ªç”¨é€—å· "," éš”å¼€
        """
        if isinstance(payload, (int, str)):
            payload = {"tid": payload}
        elif not isinstance(payload, dict):
            payload = {"tid": ",".join(map(str, payload))}
        return self._request_api(
            "POST",
            "/open/rb/del",
            data=payload,
        )

    def iter_files_with_path_simple(
        self,
        path: str | int | Path | PathLike,
        /,
        qps: int = 5,
        page_size: int = 1150,
        max_workers: int = 10,
        type: Optional[int] = None,
        suffix: Optional[str] = None,
        asc: int = 1,
        order: Literal[
            "file_name", "file_size", "user_utime", "file_type"
        ] = "file_name",
    ) -> Iterator[Dict]:
        """
        éå†ç›®å½•æ ‘è¿­ä»£æ–‡ä»¶ä¿¡æ¯å¯¹è±¡

        .. attention::
            æ­¤æ–¹æ³•å…·æœ‰å±€é™æ€§
            ç½‘ç›˜ç›®å½•æ ¼å¼å¿…é¡»æ˜¯æ–‡ä»¶å±‚çº§å‰éƒ½æ˜¯æ–‡ä»¶å¤¹ï¼Œä¸”æ–‡ä»¶å±‚çº§å†…ä¸åŒ…å«æ–‡ä»¶å¤¹
            é€‚ç”¨äºæ ‡å‡†ç”µå½±åº“ï¼ŒéŸ³ä¹åº“ç­‰

        :param path: è¿­ä»£ç›®å½•ï¼ˆå¯é€‰ç›®å½•è·¯å¾„ï¼Œcidï¼‰
        :param qps: æ¯ç§’è¯·æ±‚æ•°
        :param page_size: åˆ†é¡µå¤§å°
        :param max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        :param type: æ–‡ä»¶ç±»å‹ï¼›1.æ–‡æ¡£ï¼›2.å›¾ç‰‡ï¼›3.éŸ³ä¹ï¼›4.è§†é¢‘ï¼›5.å‹ç¼©ï¼›6.åº”ç”¨ï¼›7.ä¹¦ç±
        :param suffix: åŒ¹é…æ–‡ä»¶åç¼€å
        :param asc: å‡åºæ’åˆ—ã€‚0: å¦ï¼Œ1: æ˜¯
        :param order: æ’åº

            - "file_name": æ–‡ä»¶å
            - "file_size": æ–‡ä»¶å¤§å°
            - "file_type": æ–‡ä»¶ç§ç±»
            - "user_utime": ä¿®æ”¹æ—¶é—´

        :return: è¿­ä»£å™¨ï¼Œæ–‡ä»¶æˆ–æ–‡ä»¶å¤¹ä¿¡æ¯
        """
        rate_limiter = RateLimiter(qps)
        dir_nodes: Dict[int, DirNode] = {}
        need_parent_id_set: Set[int] = set()
        files_info_path = configer.PLUGIN_TEMP_PATH / "u115_iter_files_simple"
        if files_info_path.exists():
            rmtree(files_info_path)
        files_info: Deque = Deque(directory=files_info_path)

        def _job(
            cid: int, offset: int, show_dir: bool | None
        ) -> List[Tuple[int, str, int]]:
            rate_limiter.acquire()
            payload = {
                "cid": cid,
                "limit": page_size,
                "o": order,
                "asc": asc,
                "type": type,
                "suffix": suffix,
                "offset": offset,
            }
            if show_dir is not None:
                payload["show_dir"] = 1 if show_dir else 0
            resp = self._request_api(
                "GET",
                "/open/ufile/files",
                params=payload,
            )
            items = resp.get("data", [])
            count = resp.get("count", 0)
            sub_dirs_to_scan = []
            pre_sub_dirs_to_scan = []
            for attr in items:
                attr = normalize_attr(attr)
                files_info.append(attr)
                if attr.get("is_dir"):
                    dir_nodes[attr["id"]] = DirNode(
                        name=attr["name"], parent_id=attr["parent_id"]
                    )
                    if attr["id"] not in need_parent_id_set:
                        pre_sub_dirs_to_scan.append(attr["id"])
                    else:
                        try:
                            need_parent_id_set.remove(attr["id"])
                        except KeyError:
                            pass
                if show_dir is None:
                    need_parent_id_set.add(attr["parent_id"])
            if need_parent_id_set and pre_sub_dirs_to_scan:
                for _id in pre_sub_dirs_to_scan:
                    sub_dirs_to_scan.append((_id, 0, True))
            if show_dir is None:
                if (offset // page_size) % 5 == 0:
                    for i in range(1, 6):
                        if offset + page_size * i < count:
                            sub_dirs_to_scan.append(
                                (cid, offset + page_size * i, show_dir)
                            )
            else:
                new_offset = offset + len(items)
                if new_offset < count and len(items) > 0:
                    sub_dirs_to_scan.append((cid, new_offset, show_dir))
            return sub_dirs_to_scan

        if isinstance(path, (Path, PathLike)):
            storage_chain = StorageChain()
            file_item = storage_chain.get_file_item(storage="u115", path=Path(path))
            if not file_item:
                raise CanNotFindPathToCid("æ— æ³•è·å–ç›®å½•ä¿¡æ¯")
            path = file_item.fileid
        initial_cid = int(path)
        full_path = ""
        if initial_cid != 0:
            resp = self._request_api(
                "GET",
                "/open/folder/get_info",
                params={"file_id": initial_cid},
            )
            if not resp:
                raise OSError("è·å–ç›®å½•ä¿¡æ¯å¤±è´¥")
            data: Dict = resp.get("data")
            paths: List[Dict] = data.get("paths")
            if len(paths) > 1:
                for p in paths[1:]:
                    full_path += f"/{p.get('file_name')}"
            full_path += f"/{data.get('file_name')}"
        if page_size <= 0 or page_size > 1_150:
            page_size = 1_150

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            pending_futures: Set[Future] = set()
            # æ–‡ä»¶åˆ—è¡¨æ‹‰å–
            initial_future = executor.submit(_job, initial_cid, 0, None)
            pending_futures.add(initial_future)
            while pending_futures:
                for future in as_completed(pending_futures):
                    pending_futures.remove(future)
                    try:
                        sub_dirs = future.result()
                        for task_args in sub_dirs:
                            new_future = executor.submit(_job, *task_args)
                            pending_futures.add(new_future)
                    except Exception as e:
                        for f in pending_futures:
                            f.cancel()
                        executor.shutdown(wait=False, cancel_futures=True)
                        raise e
                    break
            # æ–‡ä»¶å¤¹åˆ—è¡¨æ‹‰å–
            try:
                need_parent_id_set.remove(initial_cid)
            except KeyError:
                pass
            initial_future = executor.submit(_job, initial_cid, 0, True)
            pending_futures.add(initial_future)
            while pending_futures:
                for future in as_completed(pending_futures):
                    pending_futures.remove(future)
                    try:
                        sub_dirs = future.result()
                        for task_args in sub_dirs:
                            new_future = executor.submit(_job, *task_args)
                            pending_futures.add(new_future)
                    except Exception as e:
                        for f in pending_futures:
                            f.cancel()
                        executor.shutdown(wait=False, cancel_futures=True)
                        raise e
                    break
        if need_parent_id_set:
            raise OSError("æ‹‰å–ä¿¡æ¯ä¸å®Œæ•´ï¼Œæ­¤ç›®å½•ç»“æ„æ— æ³•ä½¿ç”¨è¯¥å‡½æ•°")
        for file in files_info:
            pid = file["parent_id"]
            path = f"/{file['name']}"
            while pid != initial_cid:
                path = f"/{dir_nodes[pid].name}" + path
                pid = dir_nodes[pid].parent_id
            file["relpath"] = path
            file["path"] = full_path + path
            yield file
        if files_info_path.exists():
            rmtree(files_info_path)

    def iter_files_with_path(
        self,
        path: str | int | Path | PathLike,
        /,
        qps: int = 5,
        page_size: int = 1150,
        max_workers: int = 10,
        type: Optional[int] = None,
        suffix: Optional[str] = None,
        asc: int = 1,
        order: Literal[
            "file_name", "file_size", "user_utime", "file_type"
        ] = "file_name",
    ) -> Iterator[Dict]:
        """
        éå†ç›®å½•æ ‘è¿­ä»£æ–‡ä»¶ä¿¡æ¯å¯¹è±¡

        .. attention::
            æ­¤æ–¹æ³•é€šè¿‡é€’å½’æ‹‰å–æ‰€æœ‰æ–‡ä»¶ä¿¡æ¯
            æ‹‰å–é€Ÿåº¦ä¸æ–‡ä»¶å¤¹æ•°æˆæ­£æ¯”

        :param path: è¿­ä»£ç›®å½•ï¼ˆå¯é€‰ç›®å½•è·¯å¾„ï¼Œcidï¼‰
        :param qps: æ¯ç§’è¯·æ±‚æ•°
        :param page_size: åˆ†é¡µå¤§å°
        :param max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        :param type: æ–‡ä»¶ç±»å‹ï¼›1.æ–‡æ¡£ï¼›2.å›¾ç‰‡ï¼›3.éŸ³ä¹ï¼›4.è§†é¢‘ï¼›5.å‹ç¼©ï¼›6.åº”ç”¨ï¼›7.ä¹¦ç±
        :param suffix: åŒ¹é…æ–‡ä»¶åç¼€å
        :param asc: å‡åºæ’åˆ—ã€‚0: å¦ï¼Œ1: æ˜¯
        :param order: æ’åº

            - "file_name": æ–‡ä»¶å
            - "file_size": æ–‡ä»¶å¤§å°
            - "file_type": æ–‡ä»¶ç§ç±»
            - "user_utime": ä¿®æ”¹æ—¶é—´

        :return: è¿­ä»£å™¨ï¼Œæ–‡ä»¶æˆ–æ–‡ä»¶å¤¹ä¿¡æ¯
        """
        rate_limiter = RateLimiter(qps)

        def _job(
            cid: int,
            path_prefix: str,
            offset: int,
        ) -> Tuple[List[Dict[str, Any]], List[Tuple[int, str, int]]]:
            rate_limiter.acquire()
            payload = {
                "cid": cid,
                "limit": page_size,
                "o": order,
                "asc": asc,
                "type": type,
                "suffix": suffix,
                "offset": offset,
                "show_dir": 1,
            }
            resp = self._request_api(
                "GET",
                "/open/ufile/files",
                params=payload,
            )
            items = resp.get("data", [])
            count = resp.get("count", 0)
            files_found = []
            sub_dirs_to_scan = []
            for attr in items:
                attr = normalize_attr(attr)
                path = (
                    f"{path_prefix}/{attr['name']}"
                    if path_prefix
                    else f"/{attr['name']}"
                )
                attr["relpath"] = path
                attr["path"] = full_path + path
                files_found.append(attr)
                if attr.get("is_dir"):
                    sub_dirs_to_scan.append((int(attr["id"]), path, 0))
            new_offset = offset + len(items)
            if new_offset < count and len(items) > 0:
                sub_dirs_to_scan.append((cid, path_prefix, new_offset))
            return files_found, sub_dirs_to_scan

        if isinstance(path, (Path, PathLike)):
            storage_chain = StorageChain()
            file_item = storage_chain.get_file_item(storage="u115", path=Path(path))
            if not file_item:
                raise CanNotFindPathToCid("æ— æ³•è·å–ç›®å½•ä¿¡æ¯")
            path = file_item.fileid
        initial_cid = int(path)
        full_path = ""
        if initial_cid != 0:
            resp = self._request_api(
                "GET",
                "/open/folder/get_info",
                params={"file_id": initial_cid},
            )
            if not resp:
                raise OSError("è·å–ç›®å½•ä¿¡æ¯å¤±è´¥")
            data: Dict = resp.get("data")
            paths: List[Dict] = data.get("paths")
            if len(paths) > 1:
                for p in paths[1:]:
                    full_path += f"/{p.get('file_name')}"
            full_path += f"/{data.get('file_name')}"
        if page_size <= 0 or page_size > 1_150:
            page_size = 1_150

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            pending_futures: Set[Future] = set()
            initial_future = executor.submit(_job, initial_cid, "", 0)
            pending_futures.add(initial_future)
            while pending_futures:
                for future in as_completed(pending_futures):
                    pending_futures.remove(future)
                    try:
                        files, sub_dirs = future.result()
                        for file_info in files:
                            yield file_info
                        for task_args in sub_dirs:
                            new_future = executor.submit(_job, *task_args)
                            pending_futures.add(new_future)
                    except Exception as e:
                        for f in pending_futures:
                            f.cancel()
                        executor.shutdown(wait=False, cancel_futures=True)
                        raise e
                    break

    def iter_files_with_path_inc(
        self,
        path: str | int | Path | PathLike,
        /,
        db: Optional[OpenFileOper] = None,
        cache_path: Optional[Path | str | PathLike] = None,
        mode: Literal["add", "remove"] = "add",
        qps: int = 5,
        page_size: int = 1150,
        max_workers: int = 10,
        type: Optional[int] = None,
        suffix: Optional[str] = None,
        asc: int = 1,
        order: Literal[
            "file_name", "file_size", "user_utime", "file_type"
        ] = "file_name",
    ) -> Iterator[Dict]:
        """
        ä¾æ®æœ¬åœ°åª’ä½“æ–‡ä»¶æ•°æ®å¢é‡è¿­ä»£æ–‡ä»¶ä¿¡æ¯å¯¹è±¡

        :param path: è¿­ä»£ç›®å½•ï¼ˆå¯é€‰ç›®å½•è·¯å¾„ï¼Œcidï¼‰
        :param db: æ•°æ®åº“æ“ä½œç±»
        :param cache_path: ä¿¡æ¯ç¼“å­˜è·¯å¾„ï¼Œå¦‚æœä¼ å…¥è·¯å¾„å­˜åœ¨åˆ™è·³è¿‡æ‹‰å–æ•°æ®ï¼Œä¸å­˜åœ¨åˆ™è‡ªåŠ¨æ‹‰å–æ•°æ®
        :param mode: æ•°æ®å¤„ç†æ¨¡å¼
        :param qps: æ¯ç§’è¯·æ±‚æ•°
        :param page_size: åˆ†é¡µå¤§å°
        :param max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        :param type: æ–‡ä»¶ç±»å‹ï¼›1.æ–‡æ¡£ï¼›2.å›¾ç‰‡ï¼›3.éŸ³ä¹ï¼›4.è§†é¢‘ï¼›5.å‹ç¼©ï¼›6.åº”ç”¨ï¼›7.ä¹¦ç±
        :param suffix: åŒ¹é…æ–‡ä»¶åç¼€å
        :param asc: å‡åºæ’åˆ—ã€‚0: å¦ï¼Œ1: æ˜¯
        :param order: æ’åº

            - "file_name": æ–‡ä»¶å
            - "file_size": æ–‡ä»¶å¤§å°
            - "file_type": æ–‡ä»¶ç§ç±»
            - "user_utime": ä¿®æ”¹æ—¶é—´

        :return: è¿­ä»£å™¨ï¼Œæ–‡ä»¶æˆ–æ–‡ä»¶å¤¹ä¿¡æ¯
        """
        rate_limiter = RateLimiter(qps)
        lock = Lock()

        if isinstance(path, (Path, PathLike)):
            storage_chain = StorageChain()
            file_item = storage_chain.get_file_item(storage="u115", path=Path(path))
            if not file_item:
                raise CanNotFindPathToCid("æ— æ³•è·å–ç›®å½•ä¿¡æ¯")
            path = file_item.fileid
        initial_cid = int(path)

        full_path = ""
        if initial_cid != 0:
            resp = self._request_api(
                "GET",
                "/open/folder/get_info",
                params={"file_id": initial_cid},
            )
            if not resp:
                raise OSError("è·å–ç›®å½•ä¿¡æ¯å¤±è´¥")
            data: Dict = resp.get("data")
            paths: List[Dict] = data.get("paths")
            if len(paths) > 1:
                for p in paths[1:]:
                    full_path += f"/{p.get('file_name')}"
            full_path += f"/{data.get('file_name')}"

        if page_size <= 0 or page_size > 1_150:
            page_size = 1_150

        if not db:
            db = OpenFileOper()

        if not cache_path:
            cache_path = configer.PLUGIN_TEMP_PATH / "u115_iter_files_inc"
        cache: Deque = Deque(directory=cache_path)

        if len(cache) == 0:
            """
            cache ç£ç›˜ç¼“å­˜ ä¸ file_ids åˆ—è¡¨ index ç›¸å¯¹åº”
            cache[-2] å‚¨å­˜ä¸º file_ids åˆ—è¡¨
            cache[-1] å‚¨å­˜ parent_id_paths é”®å€¼å¯¹
            """
            need_parent_id_set: Set[int] = set()
            db_parent_id_set: Set[int] = db.get_all_id("folder")
            file_ids: List[int] = []
            parent_id_paths: Dict[int, str] = {}

            def _pull_files_job(
                cid: int,
                offset: int,
            ) -> List[Tuple[int, str, int]]:
                rate_limiter.acquire()
                payload = {
                    "cid": cid,
                    "limit": page_size,
                    "offset": offset,
                    "o": order,
                    "asc": asc,
                    "type": type,
                    "suffix": suffix,
                }
                resp = self._request_api(
                    "GET",
                    "/open/ufile/files",
                    params=payload,
                )
                items = resp.get("data", [])
                count = resp.get("count", 0)
                sub_dirs_to_scan = []
                for attr in items:
                    attr = normalize_attr(attr)
                    with lock:
                        cache.append(attr)
                        file_ids.append(attr["id"])
                        need_parent_id_set.add(attr["parent_id"])
                if (offset // page_size) % 5 == 0:
                    for i in range(1, 6):
                        if offset + page_size * i < count:
                            sub_dirs_to_scan.append((cid, offset + page_size * i))
                return sub_dirs_to_scan

            def _get_path_job(file_id: int) -> List[Dict]:
                if file_id not in find_parent_id_set:
                    return []
                rate_limiter.acquire()
                return_paths = []
                resp = self._request_api(
                    "GET",
                    "/open/folder/get_info",
                    params={
                        "file_id": file_id,
                    },
                )
                data = resp.get("data")
                path = ""
                paths: List[Dict] = data.get("paths")
                if len(paths) > 1:
                    for i in range(1, len(paths)):
                        p = paths[i]
                        path += f"/{p.get('file_name')}"
                        if p.get("file_id") not in db_parent_id_set:
                            return_paths.append(
                                {
                                    "id": p.get("file_id"),
                                    "parent_id": paths[i - 1].get("file_id"),
                                    "name": p.get("file_name"),
                                    "path": path,
                                }
                            )
                            if p.get("file_id") in find_parent_id_set:
                                try:
                                    find_parent_id_set.remove(p.get("file_id"))
                                except KeyError:
                                    pass
                path += f"/{data.get('file_name')}"
                return_paths.append(
                    {
                        "id": file_id,
                        "parent_id": paths[-1].get("file_id"),
                        "name": data.get("file_name"),
                        "path": path,
                    }
                )
                parent_id_paths[file_id] = path
                try:
                    find_parent_id_set.remove(file_id)
                except KeyError:
                    pass
                return return_paths

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                pending_futures: Set[Future] = set()
                initial_future = executor.submit(_pull_files_job, initial_cid, 0)
                pending_futures.add(initial_future)
                while pending_futures:
                    for future in as_completed(pending_futures):
                        pending_futures.remove(future)
                        try:
                            sub_dirs = future.result()
                            for task_args in sub_dirs:
                                new_future = executor.submit(
                                    _pull_files_job, *task_args
                                )
                                pending_futures.add(new_future)
                        except Exception as e:
                            for f in pending_futures:
                                f.cancel()
                            executor.shutdown(wait=False, cancel_futures=True)
                            raise e
                        break
                cache.append(file_ids)  # cache[-2]

                find_parent_id_set: Set[int] = need_parent_id_set - db_parent_id_set

                if find_parent_id_set:
                    results = executor.map(_get_path_job, find_parent_id_set)
                    for items in batched(results, 8_000):
                        datas: List[Dict] = []
                        for item in items:
                            datas.extend(item)
                        db.upsert_batch(datas, "folder")
                cache.append(parent_id_paths)  # cache[-1]

        file_ids_list: List[int] = cache[-2]
        parent_id_paths_dict: Dict[int, str] = cache[-1]
        db_file_ids: Set[int] = db.get_all_id("file")
        if mode == "add":
            find_file_ids: Set[int] = set(file_ids_list) - db_file_ids
            if find_file_ids:
                for file_id in find_file_ids:
                    index = file_ids_list.index(file_id)
                    item = cache[index]
                    try:
                        path = (
                            f"{parent_id_paths_dict[item['parent_id']]}/{item['name']}"
                        )
                    except KeyError:
                        path = f"{db.get_parent_path_by_id(item['parent_id'])}/{item['name']}"
                    item["path"] = path
                    yield item
        else:
            find_file_ids: Set[int] = db_file_ids - set(file_ids_list)
            if find_file_ids:
                yield from db.get_files_info_by_id(find_file_ids)
